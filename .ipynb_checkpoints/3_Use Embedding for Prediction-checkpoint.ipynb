{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0049a4d",
   "metadata": {},
   "source": [
    "# Code to test HAIM framework on a specific application using HAIM-MIMIC-MM dataset\n",
    "\n",
    "This notebook is dedicated to demonstrate how an user can use the embedding csv files to make predictions of their task of interest. We here choose to predict a patient's mortality status in the next 48 hours as a demonstration. A patient's death status is defined by their hospital discharge location, if it is morgue, hospice or expired, we consider the patient has died. \n",
    "\n",
    "### Project Info\n",
    " ->Copyright 2020 (Last Update: June 07, 2022)\n",
    " \n",
    " -> Authors: \n",
    "        Luis R Soenksen (<soenksen@mit.edu>),\n",
    "        Yu Ma (<midsumer@mit.edu>),\n",
    "        Cynthia Zeng (<czeng12@mit.edu>),\n",
    "        Ignacio Fuentes (<ifuentes@mit.edu>),\n",
    "        Leonard David Jean Boussioux (<leobix@mit.edu>),\n",
    "        Agni Orfanoudaki (<agniorf@mit.edu>),\n",
    "        Holly Mika Wiberg (<hwiberg@mit.edu>),\n",
    "        Michael Lingzhi Li (<mlli@mit.edu>),\n",
    "        Kimberly M Villalobos Carballo (<kimvc@mit.edu>),\n",
    "        Liangyuan Na (<lyna@mit.edu>),\n",
    "        Dimitris J Bertsimas (<dbertsim@mit.edu>),\n",
    "\n",
    "```\n",
    "**Licensed under the Apache License, Version 2.0**\n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf11828",
   "metadata": {},
   "source": [
    "### Requires \n",
    "```\n",
    " -> Previously generated pickle files from HAIM-MIMIC-MM Dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e297a4f",
   "metadata": {},
   "source": [
    "## I. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f544ae37-20b8-4e8e-9e97-68a643e3c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost==1.7.6\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 200.3 MB 224.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/abajrach/.local/lib/python3.9/site-packages (from xgboost==1.7.6) (1.19.5)\n",
      "Requirement already satisfied: scipy in /home/abajrach/.local/lib/python3.9/site-packages (from xgboost==1.7.6) (1.5.4)\n",
      "\u001b[33mWARNING: Error parsing requirements for nvidia-nvtx-cu12: [Errno 2] No such file or directory: '/home/abajrach/.local/lib/python3.9/site-packages/nvidia_nvtx_cu12-12.9.79.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.1.4\n",
      "    Uninstalling xgboost-2.1.4:\n",
      "      Successfully uninstalled xgboost-2.1.4\n",
      "Successfully installed xgboost-1.7.6\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost==1.7.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504b3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8aaa3",
   "metadata": {},
   "source": [
    "## II. Reading Data and Constructing Prediction Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e8d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   haim_id  de_0  de_1  de_2  de_3  de_4      vd_0      vd_1      vd_2  \\\n",
      "0       33  81.0     0     0     0     0  0.001103  0.095721  0.297696   \n",
      "1       47  89.0     0     0     0     0  0.002032  0.026490  0.366769   \n",
      "2       96  86.0     0     0     0     0  0.002315  0.139908  0.113992   \n",
      "3       96  86.0     0     0     0     0  0.000000  0.134944  0.195479   \n",
      "4      110  53.0     0     0     0     0  0.000000  0.044526  0.280396   \n",
      "\n",
      "       vd_3  ...  Fracture  Lung Lesion  Lung Opacity  No Finding  \\\n",
      "0  0.003334  ...       0.0          0.0           1.0         0.0   \n",
      "1  0.014916  ...       0.0          0.0           0.0         1.0   \n",
      "2  0.004673  ...       0.0          0.0           0.0         1.0   \n",
      "3  0.015708  ...       0.0          0.0           0.0         0.0   \n",
      "4  0.007477  ...       0.0          0.0           1.0         0.0   \n",
      "\n",
      "   Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  Support Devices  \\\n",
      "0               1.0            0.0        1.0           0.0              1.0   \n",
      "1               0.0            0.0        0.0           0.0              1.0   \n",
      "2               0.0            0.0        0.0           0.0              0.0   \n",
      "3               0.0            0.0        1.0           0.0              0.0   \n",
      "4               0.0            0.0        0.0           0.0              1.0   \n",
      "\n",
      "   y  \n",
      "0  1  \n",
      "1  1  \n",
      "2  1  \n",
      "3  1  \n",
      "4  1  \n",
      "\n",
      "[5 rows x 2556 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = \"fusion_emb.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "# Create filtered copies safely\n",
    "df_death_small48 = df[(df['img_length_of_stay'] < 48) & (df['death_status'] == 1)].copy()\n",
    "df_alive_big48 = df[(df['img_length_of_stay'] >= 48) & (df['death_status'] == 0)].copy()\n",
    "df_death_big48 = df[(df['img_length_of_stay'] >= 48) & (df['death_status'] == 1)].copy()\n",
    "\n",
    "# Assign target labels\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "\n",
    "# Concatenate subsets\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis=0)\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Drop unwanted columns\n",
    "df = df.drop(\n",
    "    ['img_id', 'img_charttime', 'img_deltacharttime', \n",
    "     'discharge_location', 'img_length_of_stay', 'death_status',\n",
    "     'split', 'PerformedProcedureStepDescription', 'ViewPosition', 'Support Devices', 'Fracture'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2eeac",
   "metadata": {},
   "source": [
    "## III. Training/Testing Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c366c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test shapes (718, 2554) (434, 2554) (718,) (434,)\n",
      "train set, death outcome case = 16.0, percentage = 0.02\n",
      "test set, death outcome case = 5.0, percentage = 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Start from df (already cleaned and with 'y')\n",
    "x_y = df.copy()\n",
    "x_y = x_y.astype('float32')\n",
    "\n",
    "# Drop rows with NaNs first\n",
    "x_y = x_y[~x_y.isna().any(axis=1)]\n",
    "\n",
    "# Unique IDs from clean data\n",
    "pkl_list = x_y['haim_id'].unique().tolist()\n",
    "\n",
    "# Train-test split of IDs\n",
    "train_id, test_id = train_test_split(pkl_list, test_size=0.3, random_state=seed)\n",
    "\n",
    "# Train/test sets\n",
    "train_set = x_y[x_y['haim_id'].isin(train_id)]\n",
    "test_set = x_y[x_y['haim_id'].isin(test_id)]\n",
    "\n",
    "# Features and labels\n",
    "y_train = train_set['y']\n",
    "y_test = test_set['y']\n",
    "\n",
    "x_train = train_set.drop(['y','haim_id'], axis=1)\n",
    "x_test = test_set.drop(['y','haim_id'], axis=1)\n",
    "\n",
    "print('train, test shapes', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print('train set, death outcome case = %s, percentage = %.2f' % (y_train.sum(),  y_train.sum()/len(y_train)))\n",
    "print('test set, death outcome case = %s, percentage = %.2f' % (y_test.sum(),  y_test.sum()/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3504a",
   "metadata": {},
   "source": [
    "## IV: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84dd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "gs_metric = 'roc_auc'\n",
    "param_grid = {'max_depth': [5, 6, 7, 8],\n",
    "             'n_estimators': [200, 300],\n",
    "             'learning_rate': [0.3, 0.1, 0.05],\n",
    "             }\n",
    "\n",
    "est = xgb.XGBClassifier(verbosity=0, scale_pos_weight = (len(y_train) - sum(y_train))/sum(y_train), seed = 42,\n",
    "                        eval_metric='logloss')\n",
    "\n",
    "gs = GridSearchCV(estimator = est, param_grid=param_grid, scoring=gs_metric, cv= cv_folds)\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "y_pred_prob_train = gs.predict_proba(x_train)\n",
    "y_pred_train = gs.predict(x_train)\n",
    "\n",
    "y_pred_prob_test = gs.predict_proba(x_test)\n",
    "y_pred_test = gs.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d930832",
   "metadata": {},
   "source": [
    "## V:Reporting Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c59da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "f1_train = metrics.f1_score(y_train, y_pred_train, average='macro')\n",
    "accu_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "accu_bl_train = metrics.balanced_accuracy_score(y_train, y_pred_train)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_pred_prob_train[:, 1])\n",
    "conf_matrix_train = metrics.confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a1a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set is: 1.0\n",
      "Accuracy for Training Set is: 1.0\n",
      "Balanced Accuracy for Training Set is: 1.0\n",
      "AUC for Training Set is: 1.0\n",
      "Confusion Matrix for Training Set is: [[702   0]\n",
      " [  0  16]]\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for Training Set is: {f1_train}')\n",
    "print(f'Accuracy for Training Set is: {accu_train}')\n",
    "print(f'Balanced Accuracy for Training Set is: {accu_bl_train}')\n",
    "print(f'AUC for Training Set is: {auc_train}')\n",
    "print(f'Confusion Matrix for Training Set is: {conf_matrix_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57bfc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = metrics.f1_score(y_test, y_pred_test, average='macro')\n",
    "accu_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "accu_bl_test = metrics.balanced_accuracy_score(y_test, y_pred_test)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_pred_prob_test[:, 1])\n",
    "conf_matrix_test = metrics.confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0670df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Testing Set is: 0.49710312862108924\n",
      "Accuracy for Testing Set is: 0.988479262672811\n",
      "Balanced Accuracy for Testing Set is: 0.5\n",
      "AUC for Testing Set is: 0.7445221445221445\n",
      "Confusion Matrix for Testing Set is: [[429   0]\n",
      " [  5   0]]\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for Testing Set is: {f1_test}')\n",
    "print(f'Accuracy for Testing Set is: {accu_test}')\n",
    "print(f'Balanced Accuracy for Testing Set is: {accu_bl_test}')\n",
    "print(f'AUC for Testing Set is: {auc_test}')\n",
    "print(f'Confusion Matrix for Testing Set is: {conf_matrix_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404273f2-d65c-4a72-a188-6f3c5d30d8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
