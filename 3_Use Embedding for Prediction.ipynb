{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0049a4d",
   "metadata": {},
   "source": [
    "# Code to test HAIM framework on a specific application using HAIM-MIMIC-MM dataset\n",
    "\n",
    "This notebook is dedicated to demonstrate how an user can use the embedding csv files to make predictions of their task of interest. We here choose to predict a patient's mortality status in the next 48 hours as a demonstration. A patient's death status is defined by their hospital discharge location, if it is morgue, hospice or expired, we consider the patient has died. \n",
    "\n",
    "### Project Info\n",
    " ->Copyright 2020 (Last Update: June 07, 2022)\n",
    " \n",
    " -> Authors: \n",
    "        Luis R Soenksen (<soenksen@mit.edu>),\n",
    "        Yu Ma (<midsumer@mit.edu>),\n",
    "        Cynthia Zeng (<czeng12@mit.edu>),\n",
    "        Ignacio Fuentes (<ifuentes@mit.edu>),\n",
    "        Leonard David Jean Boussioux (<leobix@mit.edu>),\n",
    "        Agni Orfanoudaki (<agniorf@mit.edu>),\n",
    "        Holly Mika Wiberg (<hwiberg@mit.edu>),\n",
    "        Michael Lingzhi Li (<mlli@mit.edu>),\n",
    "        Kimberly M Villalobos Carballo (<kimvc@mit.edu>),\n",
    "        Liangyuan Na (<lyna@mit.edu>),\n",
    "        Dimitris J Bertsimas (<dbertsim@mit.edu>),\n",
    "\n",
    "```\n",
    "**Licensed under the Apache License, Version 2.0**\n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf11828",
   "metadata": {},
   "source": [
    "### Requires \n",
    "```\n",
    " -> Previously generated pickle files from HAIM-MIMIC-MM Dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e297a4f",
   "metadata": {},
   "source": [
    "## I. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f544ae37-20b8-4e8e-9e97-68a643e3c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[33mWARNING: Error parsing requirements for nvidia-nvtx-cu12: [Errno 2] No such file or directory: '/home/abajrach/.local/lib/python3.9/site-packages/nvidia_nvtx_cu12-12.9.79.dist-info/METADATA'\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.9 are installed in '/home/abajrach/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.23.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.23.5\n",
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn==1.2.2\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /home/abajrach/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/abajrach/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/abajrach/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abajrach/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2) (3.6.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for nvidia-nvtx-cu12: [Errno 2] No such file or directory: '/home/abajrach/.local/lib/python3.9/site-packages/nvidia_nvtx_cu12-12.9.79.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed scikit-learn-1.2.2\n",
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost==1.7.6 in /home/abajrach/.local/lib/python3.9/site-packages (1.7.6)\n",
      "Requirement already satisfied: scipy in /home/abajrach/.local/lib/python3.9/site-packages (from xgboost==1.7.6) (1.13.1)\n",
      "Requirement already satisfied: numpy in /home/abajrach/.local/lib/python3.9/site-packages (from xgboost==1.7.6) (1.23.5)\n",
      "\u001b[33mWARNING: Error parsing requirements for nvidia-nvtx-cu12: [Errno 2] No such file or directory: '/home/abajrach/.local/lib/python3.9/site-packages/nvidia_nvtx_cu12-12.9.79.dist-info/METADATA'\u001b[0m\n",
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/abajrach/.bashrc: No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imbalanced-learn==0.12.4\n",
      "  Using cached imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/abajrach/.local/lib/python3.9/site-packages (from imbalanced-learn==0.12.4) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/abajrach/.local/lib/python3.9/site-packages (from imbalanced-learn==0.12.4) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/abajrach/.local/lib/python3.9/site-packages (from imbalanced-learn==0.12.4) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/abajrach/.local/lib/python3.9/site-packages (from imbalanced-learn==0.12.4) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/abajrach/.local/lib/python3.9/site-packages (from imbalanced-learn==0.12.4) (1.13.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for nvidia-nvtx-cu12: [Errno 2] No such file or directory: '/home/abajrach/.local/lib/python3.9/site-packages/nvidia_nvtx_cu12-12.9.79.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.8.1\n",
      "    Uninstalling imbalanced-learn-0.8.1:\n",
      "      Successfully uninstalled imbalanced-learn-0.8.1\n",
      "Successfully installed imbalanced-learn-0.12.4\n"
     ]
    }
   ],
   "source": [
    "# create new environment\n",
    "!python -m venv haim_env\n",
    "!source haim_env/bin/activate  # Linux/Mac\n",
    "# or haim_env\\Scripts\\activate  # Windows\n",
    "\n",
    "# Install compatible packages\n",
    "!pip install numpy==1.23.5\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install xgboost==1.7.6\n",
    "!pip install imbalanced-learn==0.12.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504b3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8aaa3",
   "metadata": {},
   "source": [
    "## II. Reading Data and Constructing Prediction Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e8d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   haim_id  de_0  de_1  de_2  de_3  de_4      vd_0      vd_1      vd_2  \\\n",
      "0       33  81.0     0     0     0     0  0.001103  0.095721  0.297696   \n",
      "1       47  89.0     0     0     0     0  0.002032  0.026490  0.366769   \n",
      "2       96  86.0     0     0     0     0  0.002315  0.139908  0.113992   \n",
      "3       96  86.0     0     0     0     0  0.000000  0.134944  0.195479   \n",
      "4      110  53.0     0     0     0     0  0.000000  0.044526  0.280396   \n",
      "\n",
      "       vd_3  ...  Edema  Enlarged Cardiomediastinum  Lung Lesion  \\\n",
      "0  0.003334  ...   -1.0                        -1.0          0.0   \n",
      "1  0.014916  ...    0.0                         0.0          0.0   \n",
      "2  0.004673  ...    0.0                         0.0          0.0   \n",
      "3  0.015708  ...    0.0                         0.0          0.0   \n",
      "4  0.007477  ...    0.0                         0.0          0.0   \n",
      "\n",
      "   Lung Opacity  No Finding  Pleural Effusion  Pleural Other  Pneumonia  \\\n",
      "0           1.0         0.0               1.0            0.0        1.0   \n",
      "1           0.0         1.0               0.0            0.0        0.0   \n",
      "2           0.0         1.0               0.0            0.0        0.0   \n",
      "3           0.0         0.0               0.0            0.0        1.0   \n",
      "4           1.0         0.0               0.0            0.0        0.0   \n",
      "\n",
      "   Pneumothorax  y  \n",
      "0           0.0  1  \n",
      "1           0.0  1  \n",
      "2           0.0  1  \n",
      "3           0.0  1  \n",
      "4           0.0  1  \n",
      "\n",
      "[5 rows x 2554 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = \"fusion_emb.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "# Create filtered copies safely\n",
    "df_death_small48 = df[(df['img_length_of_stay'] < 48) & (df['death_status'] == 1)].copy()\n",
    "df_alive_big48 = df[(df['img_length_of_stay'] >= 48) & (df['death_status'] == 0)].copy()\n",
    "df_death_big48 = df[(df['img_length_of_stay'] >= 48) & (df['death_status'] == 1)].copy()\n",
    "\n",
    "# Assign target labels\n",
    "df_death_small48['y'] = 1\n",
    "df_alive_big48['y'] = 0\n",
    "df_death_big48['y'] = 0\n",
    "\n",
    "# Concatenate subsets\n",
    "df = pd.concat([df_death_small48, df_alive_big48, df_death_big48], axis=0)\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Drop unwanted columns\n",
    "df = df.drop(\n",
    "    ['img_id', 'img_charttime', 'img_deltacharttime', \n",
    "     'discharge_location', 'img_length_of_stay', 'death_status',\n",
    "     'split', 'PerformedProcedureStepDescription', 'ViewPosition', 'Support Devices', 'Fracture'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2eeac",
   "metadata": {},
   "source": [
    "## III. Training/Testing Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c366c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test shapes (718, 2552) (434, 2552) (718,) (434,)\n",
      "train set, death outcome case = 16.0, percentage = 0.02\n",
      "test set, death outcome case = 5.0, percentage = 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Start from df (already cleaned and with 'y')\n",
    "x_y = df.copy()\n",
    "x_y = x_y.astype('float32')\n",
    "\n",
    "# Drop rows with NaNs first\n",
    "x_y = x_y[~x_y.isna().any(axis=1)]\n",
    "\n",
    "# Unique IDs from clean data\n",
    "pkl_list = x_y['haim_id'].unique().tolist()\n",
    "\n",
    "# Train-test split of IDs\n",
    "train_id, test_id = train_test_split(pkl_list, test_size=0.3, random_state=seed)\n",
    "\n",
    "# Train/test sets\n",
    "train_set = x_y[x_y['haim_id'].isin(train_id)]\n",
    "test_set = x_y[x_y['haim_id'].isin(test_id)]\n",
    "\n",
    "# Features and labels\n",
    "y_train = train_set['y']\n",
    "y_test = test_set['y']\n",
    "\n",
    "x_train = train_set.drop(['y','haim_id'], axis=1)\n",
    "x_test = test_set.drop(['y','haim_id'], axis=1)\n",
    "\n",
    "print('train, test shapes', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print('train set, death outcome case = %s, percentage = %.2f' % (y_train.sum(),  y_train.sum()/len(y_train)))\n",
    "print('test set, death outcome case = %s, percentage = %.2f' % (y_test.sum(),  y_test.sum()/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2071e32b-de50-4cb7-8da4-e7f9504680c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total explained variance with 100 components: 0.90\n",
      "Shape after PCA: (718, 100) (434, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Scale features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# 2. Apply PCA\n",
    "n_components = 100  # choose number of components or explain variance threshold\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "x_train = pca.fit_transform(x_train_scaled)\n",
    "x_test = pca.transform(x_test_scaled)\n",
    "\n",
    "# Optional: check explained variance\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Total explained variance with {n_components} components: {explained_var:.2f}\")\n",
    "print(\"Shape after PCA:\", x_train.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8034b21-0f28-4ef0-beb0-c753471513e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3504a",
   "metadata": {},
   "source": [
    "## IV: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84dd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "gs_metric = 'roc_auc'\n",
    "param_grid = {'max_depth': [5, 6, 7, 8],\n",
    "             'n_estimators': [200, 300],\n",
    "             'learning_rate': [0.3, 0.1, 0.05],\n",
    "             }\n",
    "\n",
    "est = xgb.XGBClassifier(verbosity=0, scale_pos_weight = (len(y_train) - sum(y_train))/sum(y_train), seed = 42,\n",
    "                        eval_metric='logloss')\n",
    "\n",
    "gs = GridSearchCV(estimator = est, param_grid=param_grid, scoring=gs_metric, cv= cv_folds)\n",
    "gs.fit(x_train, y_train)\n",
    "\n",
    "y_pred_prob_train = gs.predict_proba(x_train)\n",
    "y_pred_train = gs.predict(x_train)\n",
    "\n",
    "y_pred_prob_test = gs.predict_proba(x_test)\n",
    "y_pred_test = gs.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d930832",
   "metadata": {},
   "source": [
    "## V:Reporting Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c59da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "f1_train = metrics.f1_score(y_train, y_pred_train, average='macro')\n",
    "accu_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "accu_bl_train = metrics.balanced_accuracy_score(y_train, y_pred_train)\n",
    "auc_train = metrics.roc_auc_score(y_train, y_pred_prob_train[:, 1])\n",
    "conf_matrix_train = metrics.confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a1a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Training Set is: 1.0\n",
      "Accuracy for Training Set is: 1.0\n",
      "Balanced Accuracy for Training Set is: 1.0\n",
      "AUC for Training Set is: 1.0\n",
      "Confusion Matrix for Training Set is: [[702   0]\n",
      " [  0 702]]\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for Training Set is: {f1_train}')\n",
    "print(f'Accuracy for Training Set is: {accu_train}')\n",
    "print(f'Balanced Accuracy for Training Set is: {accu_bl_train}')\n",
    "print(f'AUC for Training Set is: {auc_train}')\n",
    "print(f'Confusion Matrix for Training Set is: {conf_matrix_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bfc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = metrics.f1_score(y_test, y_pred_test, average='macro')\n",
    "accu_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "accu_bl_test = metrics.balanced_accuracy_score(y_test, y_pred_test)\n",
    "auc_test = metrics.roc_auc_score(y_test, y_pred_prob_test[:, 1])\n",
    "conf_matrix_test = metrics.confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0670df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Testing Set is: 0.49710312862108924\n",
      "Accuracy for Testing Set is: 0.988479262672811\n",
      "Balanced Accuracy for Testing Set is: 0.5\n",
      "AUC for Testing Set is: 0.7263403263403263\n",
      "Confusion Matrix for Testing Set is: [[429   0]\n",
      " [  5   0]]\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for Testing Set is: {f1_test}')\n",
    "print(f'Accuracy for Testing Set is: {accu_test}')\n",
    "print(f'Balanced Accuracy for Testing Set is: {accu_bl_test}')\n",
    "print(f'AUC for Testing Set is: {auc_test}')\n",
    "print(f'Confusion Matrix for Testing Set is: {conf_matrix_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404273f2-d65c-4a72-a188-6f3c5d30d8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
